1. Mug up square root and sqaure of numbers

APTI
————
1. Complete all scientist papers
2. Practice from RS agarwal for all chapters
3. Practice puzzles

GK
————
- Science Daily
- news.mit.edu
- https://www.newscientist.com
- http://www.sciencemag.org/

- Turing machine  — > done(news)
- Automata theory  — > done(news)
- General  Theory of Relativity  ——> done(news)
- Special  Theory of Relativity ——> done
- Big bang theory/black hole - Stephen Hawking
- String theory
- Game theory  —> done(news)
- Prime numbers   —> done(news)
- Fibonacci series — > done(news)
- Quantum theory/mechanics  -> done(news)
- Quantum compter —> done
- Crypto currency —> done(news)
- AI/Robotics
    - Artificial neural network  —> done
    - Recurrent neural network  —> done
    - Convolutional neural network —> Done
    - Linear regression —> done
    - Logistic regression  —> done
    - Support vector machines  —> done
    - Decision trees  —> done
    - Ensemble learning and random forests —> done
    - dimensionality reduction  —> done
    - Reinforcement learning —> done
- Networking/Web/Compiler/Parser
- Isac assimov—> done



GENERAL THEORY OF RELATIVITY
—————————————————————————


LIGO , VIRGO, TAMA3000 and GEO600, - To detect gravitational waves predicted by theory of relativity 

LISA - the Laser Interferometer Space Antenna, is currently under development by ESA to detect gravitational waves more accurately

Weber bars- First gravitational wave detector

LIGO made the first direct detection of gravitational waves on 14 September 2015

quark are the fundamental constituent of matter.Quarks combine to form composite particles. Most stable are neutron and proton and baryons

hadrons also include mesons such as the pion and kaon	 


An episode of the Russian science-fiction novel Space Apprentice by Arkady and Boris Strugatsky shows the experiment monitoring the propagation of gravitational waves at the expense of annihilating a chunk of asteroid 15 Eunomia the size of Mount Everest.[88]
In Stanislaw Lem's Fiasco, a "gravity gun" or "gracer" (gravity amplification by collimated emission of resonance) is used to reshape a collapsar, so that the protagonists can exploit the extreme relativistic effects and make an interstellar journey.
In Greg Egan's Diaspora, the analysis of a gravitational wave signal from the inspiral of a nearby binary neutron star reveals that its collision and merger is imminent, implying a large gamma-ray burst is going to impact the Earth.
 General Theory of relativity by Einstein in 1915.

Similarly, a 1919 expedition led by Eddington confirmed general relativity's prediction for the deflection of starlight by the Sun during the total solar eclipse of May 29, 1919,


Einstein's field equation - Equations describing general theory of relativity


General relativity proofs
   - Gravity waves
   - Gravitation time dialation and frequency shift of light(blueshift - decrease in wavelen and incerase in frequency which makes it blue , opposite is erdshift)
   - Grvitational lensing - Bending of light near a high mass - 


traversable worm hole - Can be visualized as a tunnel with two points in separate space time

SETI - search for intelligent life in the universe

A wormhole is a theoretical passage through space-time that could create shortcuts for long journeys across the universe. Wormholes are predicted by the theory of general relativity. But be wary: wormholes bring with them the dangers of sudden collapse, high radiation and dangerous contact with exotic matter.


a celestial object of very small radius (typically 30 km) and very high density, composed predominantly of closely packed neutrons. Neutron stars are thought to form by the gravitational collapse of the remnant of a massive star after a supernova explosion, provided that the star is insufficiently massive to produce a black hole.

The latest data presented by scientists on Higgs boson shows that separate measurements of its properties are showing two slightly different masses.
Experiment Using Photons Could Detect Quantum-Scale Black Holes



PRIME NUMBERS
——————————
 There are infinite number of primes demonstrated by euclid aroudn 300BC
 Primality testing 
          — Miller-rabin primality testing - small chance of error
                         — Determinstic but depends on unproven extended sReimann hypothesis
         — AKS primality testing(after Agarwal, kayal and saxena)
                         - Polynomial in time
                         - slower for practical usage
         — Pepin test(1877) - primality test for fermats number  which is of form 2^2^n + 1
         — Proth's theorem(1878) - primality test for proth number which is of the form k*2^n  + 1 
         — Lucas–Lehmer primality test  - Primality test for mersenne number
         — Great Internet Mersenne Prime Search(GIMPS) - Project to search for largest prime number. 
         — Solovay–Strassen primality test, developed by Robert M. Solovay and Volker Strassen, is a probabilistic test to determine if a number is composite or probably prime
         — randomized Las Vegas algorithms where the random choices made by the algorithm do not affect its final answer, such as some variations of elliptic curve primality proving

Manindra Agrawal, Neeraj Kayal, and Nitin Saxena, computer scientists at the Indian Institute of Technology Kanpur, on August 6, 2002, in a paper titled "PRIMES is in P".[1] The algorithm was the first to determine whether any given number is prime or composite within polynomial time. The authors received the 2006 Gödel Prize and the 2006 Fulkerson Prize for this work

Mersenne number 
   - Prime of the form 2^p -1 - Recently GIMPS has found so far largest Mersenne prime number of 23 million digits

Prime number theorem(PNT)
   - prime number becomes less common in larger number


 Goldbach's conjecture
    - Every even number greater than 2 is a sum of two primes
    - said  in a 1742 letter to Eule

Twince primes
    - There are infinite number of pair of primes with a even number between them

Public key cryptography
    - based on prime factorization - finding prime factor of a number is very very hard


The Rhind Mathematical Papyrus, from around 1550 BC, has Egyptian fraction expansions of different forms for prime and composite numbers

Euclid's Elements (circa 300 BC) proves the infinitude of primes and the fundamental theorem of arithmetic, and shows how to construct a perfect number from a Mersenne prime.[13] 


Another Greek invention, the Sieve of Eratosthenes, is still used to construct lists of primes for a given limit

Around 1000 AD, the Islamic mathematician Alhazen found Wilson's theorem, characterizing the prime numbers as the numbers  that evenly divide  (n-1)! +1

Another Islamic mathematician, Ibn al-Banna' al-Marrakushi, observed that the sieve of Eratosthenes can be sped up by testing only the divisors up to the square root of the largest number to be tested. Fibonacci brought the innovations from Islamic mathematics back to Europe. His book Liber Abaci (1202) was the first to describe trial division for testing primality, again using divisors only up to the square root.[15]

In 1640 Pierre de Fermat stated (without proof) Fermat's little theorem (later proved by Leibniz and Euler).


Fermat also investigated the primality of the Fermat numbers  2^2^n + 1

 Marin Mersenne studied the Mersenne primes, prime numbers of the form  2^p  - 1  with  itself a prime

Euler proved Alhazen's conjecture (now the Euclid–Euler theorem) that all even perfect numbers can be constructed from Mersenne primes

At the start of the 19th century, Legendre and Gauss conjectured that as x tends to infinity, the number of primes up to x is asymptotic to x/log(x), where  is the natural logarithm of  x.

Ideas of Riemann in his 1859 paper on the zeta-function sketched an outline for proving this

Zeta function - defines the properties of prime numbers also called Riemann hypothesis

 Riemann's outline was completed in 1896 by Hadamard and de la Vallée Poussin, and the result is now known as the prime number theorem

Another important 19th-century result was Dirichlet's theorem on arithmetic progressions, that certain arithmetic progressions contain infinitely many primes

The mathematical theory of prime numbers also moved forward with the Green–Tao theorem (2004) on long arithmetic progressions of prime numbers, and Yitang Zhang's 2013 proof that there exist infinitely many prime gaps of bounded size

 Euler's totient function - In number theory, Euler's totient function counts the positive integers up to a given integer n that are relatively prime to n

Euclid's lemma: Captures fundamental property of prime number

proof for infiniteness of prime numbers 
  -By Euclid
  -By Eular, Goldbach
  -Furstenberg's proof using general topology
  -Kummer's elegant proof

Formula for prime numbers
  -Wilson's theorem 
  -Diophantine equations
  -Mills' theorem 
  -Wright's theorem

Open questions
  -Landau's problems from 1912 are still unsolved
  -Goldbach's conjecture -  this conjecture has been verified for all numbers up to  n = 4.10^18
  -Vinogradov's theorem says that every sufficiently large odd integer can be written as a sum of three primes
  -Chen's theorem says that every sufficiently large even number can be expressed as the sum of a prime and a semiprime, the product of two primes.Also, any even integer can be written as the sum of six primes
  -The branch of number theory studying such questions is called additive number theory.
  -prime gaps, the differences between consecutive primes
  -Polignac's conjecture states more generally that for every positive integer , there are infinitely many pairs of consecutive primes that differ by .
  -Andrica's conjecture,[65] Brocard's conjecture,[66] Legendre's conjecture,[67] and Oppermann's conjecture[66] all suggest that the largest gaps between primes from  to  should be at most approximately root(n)	
  -while the much stronger Cramér conjecture sets the largest gap size at 0(	(log n)^2)
  -first Hardy–Littlewood conjecture, which can be motivated by the heuristic that the prime numbers behave similarly to a random sequence of numbers with density given by the prime number theorem	




This function is closely connected to the prime numbers and to one of the most significant unsolved problems in mathematics, the Riemann hypothesis which asks where the zeros of the Riemann zeta function  are located which is a Millennium Prize Problems

 Dirichlet's theorem on arithmetic progressions asserts that the progression contains infinitely many primes
  
Green–Tao theorem shows that there are arbitrarily long finite arithmetic progressions consisting only of primes


The Ulam spiral arranges the natural numbers in a two-dimensional grid, spiraling in concentric squares surrounding the origin with the prime numbers highlighted

The Millennium Prize Problems are seven problems in mathematics that were stated by the Clay Mathematics Institute on May 24, 2000.[1] The problems are the Birch and Swinnerton-Dyer conjecture, Hodge conjecture, Navier–Stokes existence and smoothness, P versus NP problem, Poincaré conjecture, Riemann hypothesis, and Yang–Mills existence and mass gap

Euler product is an expansion of a Dirichlet series into an infinite product indexed by prime numbers


Fermat's little theorem states that if p is a prime number, then for any integer a, the number ap − a is an integer multiple of p

Fermat's little theorem is the basis for the Fermat primality test is probablistic test to determine whether a number is probably prime or not


The p-adic order  of an integer  is the number of copies of  in the prime factorization of n

 Fermat's theorem on sums of two squares states that an odd prime p can be expressed as, p = x^2+y^2

In number theory Fermat's Last Theorem (sometimes called Fermat's conjecture, especially in older texts) states that no three positive integers a, b, and c satisfy the equation an + bn = cn for any integer value of n greater than 2. The cases n = 1 and n = 2have been known to have infinitely many solutions since antiquity


the sieve of Atkin is a modern algorithm for finding all prime numbers up to a specified integer which is better than sieve of Eratosthenes, 


Solovay–Strassen primality test, developed by Robert M. Solovay and Volker Strassen, is a probabilistic test to determine if a number is composite or probably prime


Test
Developed in
Type
Running time
Notes
References
AKS primality test
2002
deterministic


[128][131]
Elliptic curve primality proving
1977
Las Vegas
 heuristically

[129]
Miller–Rabin primality test
1980
Monte Carlo

error probability 
[132]
Solovay–Strassen primality test
1977
Monte Carlo

error probability 
[132]



Type
Prime
Number of decimal digits
Date
Found by
Mersenne prime
277,232,917 − 1
23,249,425
December 26, 2017[138]
Jonathan Pace, Great Internet Mersenne Prime Search
Proth prime (but not a Mersenne prime)
10,223 × 231,172,165 + 1
9,383,761
October 31, 2016[139]
Péter Szabolcs, PrimeGrid[140]
factorial prime
208,003! − 1
1,015,843
July 2016
Sou Fukui[141]
primorial prime[e]
1,098,133# − 1
476,311
March 2012
James P. Burt, PrimeGrid[143]
twin primes
2,996,863,034,895  × 21,290,000± 1
388,342
September 2016
Tom Greer, PrimeGrid[144]

Prime factorization methods	
   - Trial division and Pollard's rho algorithm can be used to find very small factors of n
   -  elliptic curve factorization can be effective when  has factors of moderate size elliptic curve factorization can be effective when  has factors of moderate size
   - Methods suitable for arbitrary large numbers that do not depend on the size of its factors include the quadratic sieve and general number field sieve
   - As of January 2018 the largest number known to have been factored by a general-purpose algorithm is RSA-768, which has 232 decimal digits (768 bits) and is the product of two large primes

l public-key cryptography algorithms, such as RSA and the Diffie–Hellman key exchange, are based on large prime numbers (2048-bit primes are common)

Prime numbers are frequently used for hash tables.

 Eisenstein's criterion, a test for whether a polynomial is irreducible based on divisibility of its coefficients by a prime number and its square

The evolutionary strategy used by cicadas of the genus Magicicada makes use of prime numbers


QUANTUM MECHANICS
——————————————
The first model that was able to explain the full spectrum of thermal radiation was put forward by Max Planck in 1900

James Clerk Maxwell had shown that electricity, magnetism and light are all manifestations of the same phenomenon: the electromagnetic field. Maxwell's equations,

Einstein explained the effect by postulating that a beam of light is a stream of particles ("photons") and that,

Einstein explained the effect by postulating that a beam of light is a stream of particles ("photons") and that,

In the same year, building on de Broglie's hypothesis, Erwin Schrödinger developed the equation that describes the behavior of a quantum mechanical wave


Bohr, Heisenberg and others tried to explain what these experimental results and mathematical models really mean. Their description, known as the Copenhagen interpretation of quantum mechanics, aimed to describe the nature of reality that was being probed by the measurements and described by the mathematical formulations of quantum mechanics.

	1.	The main principles of the Copenhagen interpretation are:
	2.	A system is completely described by a wave function, usually represented by the Greek letter  ("psi"). (Heisenberg)
	3.	How  changes over time is given by the Schrödinger equation.
	4.	The description of nature is essentially probabilistic. The probability of an event – for example, where on the screen a particle shows up in the two slit experiment – is related to the square of the absolute value of the amplitude of its wave function. (Born rule, due to Max Born, which gives a physical meaning to the wave function in the Copenhagen interpretation: the probability amplitude)
	5.	It is not possible to know the values of all of the properties of the system at the same time; those properties that are not known with precision must be described by probabilities. (Heisenberg's uncertainty principle)
	6.	Matter, like energy, exhibits a wave–particle duality. An experiment can demonstrate the particle-like properties of matter, or its wave-like properties; but not both at the same time. (Complementarity principle due to Bohr)
	7.	Measuring devices are essentially classical devices, and measure classical properties such as position and momentum.[37]
	8.	The quantum mechanical description of large systems should closely approximate the classical description. (Correspondence principle of Bohr and Heisenberg)


In 1927, Heisenberg proved that this last assumption is not correct.[39] Quantum mechanics shows that certain pairs of physical properties, such as for example position and speed, cannot be simultaneously measured, nor defined in operational terms, to arbitrary precision: the more precisely one property is measured, or defined in operational terms, the less precisely can the other. This statement is known as the uncertainty principle. The uncertainty principle isn't only a statement about the accuracy of our measuring equipment, but, more deeply, is about the conceptual nature of the measured quantities – the assumption that the car had simultaneously defined position and speed does not work in quantum mechanics. On a scale of cars and people, these uncertainties are negligible, but when dealing with atoms and electrons they become critical.[40]

The uncertainty principle shows mathematically that the product of the uncertainty in the position and momentum of a particle (momentum is velocity multiplied by mass) could never be less than a certain value, and that this value is related to Planck's constant.	



Within Schrödinger's picture, each electron has four properties:
	1.	An "orbital" designation, indicating whether the particle wave is one that is closer to the nucleus with less energy or one that is farther from the nucleus with more energy;
	2.	The "shape" of the orbital, spherical or otherwise;
	3.	The "inclination" of the orbital, determining the magnetic moment of the orbital around the z-axis.
	4.	The "spin" of the electron.
The collective name for these properties is the quantum state of the electron. The quantum state can be described by giving a number to each of these properties; these are known as the electron's quantum numbers. The quantum state of the electron is described by its wave function. The Pauli exclusion principle demands that no two electrons within an atom may have the same values of all four numbers.

In quantum mechanics, the Pauli equation or Schrödinger–Pauli equation is the formulation of the Schrödinger equation for spin-½particles, which takes into account the interaction of the particle's spin with an external electromagnetic field. It is the non-relativisticlimit of the Dirac equation and can be used where particles are moving at speeds much less than the speed of light, so that relativistic effects can be neglected. It was formulated by Wolfgang Pauli in 1927.[1]

The Einstein–Podolsky–Rosen paradox or the EPR paradox[1] of 1935 is a thought experiment in quantum mechanics with which Albert Einstein and his colleagues Boris Podolsky and Nathan Rosen (EPR) claimed to demonstrate that the wave function does not provide a complete description of physical reality, and hence that the Copenhagen interpretation is unsatisfactory; resolutions of the paradox have important implications for the interpretation of quantum mechanics.


In modern physics, antimatter is defined as a material composed of the antiparticle (or "partners") to the corresponding particlesof ordinary matter

The Higgs boson is an elementary particle in the Standard Model of particle physics, produced by the quantum excitation of the Higgs field,[8][9] one of the fields in particle physics theory.[9] It is named after physicist Peter Higgs, who in 1964, along with six other scientists, proposed the mechanism, which suggested the existence of such a particle. Its existence was confirmed by the ATLAS and CMS collaborations based on collisions in the LHC at CERN.


superposition is a fundamental principle of quantum mechanics. It states that, much like waves in classical physics, any two (or more) quantum states can be added together ("superposed") and the result will be another valid quantum state; and conversely, that every quantum state can be represented as a sum of two or more other distinct states. Mathematically, it refers to a property of solutions to the Schrödinger equation; since the Schrödinger equation is linear, any linear combination of solutions will also be a solution.

Quantum entanglement is a physical phenomenon which occurs when pairs or groups of particles are generated, interact, or share spatial proximity in ways such that the quantum state of each particle cannot be described independently of the state of the other(s), even when the particles are separated by a large distance—instead, a quantum state must be described for the system as a whole.


QUANTUM COMPUTER
———————————

The field of quantum computing was initiated by the work of Paul Benioff[2] and Yuri Manin in 1980,[3] Richard Feynman in 1982,[4] and David Deutsch in 1985.[5]

A way of understanding the quantum Turing machine (QTM) is that it generalizes the classical Turing machine (TM) in the same way that the quantum finite automaton (QFA) generalizes the deterministic finite automaton (DFA)

The IBM Q Experience is an online platform that gives users in the general public access to a set of IBM’s prototype quantum processors via the Cloud, an online internet forum for discussing quantum computing relevant topics, a set of tutorials on how to program the IBM Q devices, and other educational material about quantum computing. It is an example of cloud based quantum computing. As of May 2018, there are three processors on the IBM Q Experience: two 5-qubit processors and a 16-qubit processor. This service can be used to run algorithms and experiments, and explore tutorials and simulations around what might be possible with quantum computing. The site also provides an easily discoverable list of research papers published using the IBM Q Experience as an experimentation platform


IBM ILOG CPLEX Optimization Studio (often informally referred to simply as CPLEX) is an optimization software package. In 2004, the work on CPLEX earned the first INFORMS Impact Prize.


D-Wave Systems, Inc. [2] is a quantum computing company, based in Burnaby, British Columbia, Canada. D-Wave is the world's first company to sell quantum computers.[3]

Orion prototype[edit]
On February 13, 2007, D-Wave demonstrated the Orion system, running three different applications at the Computer History Museum in Mountain View, California. This marked the first public demonstration of, supposedly, a quantum computer and associated service.[citation needed]


2009 Google demonstration[edit]
On December 8, 2009, at the Neural Information Processing Systems (NIPS) conference, a Google research team led by Hartmut Neven used D-Wave's processor to train a binary image classifier.[citation needed]


D-Wave One[edit]
On May 11, 2011, D-Wave Systems announced the D-Wave One, an integrated quantum computer system running on a 128-qubit processor. The processor used in the D-Wave One code-named "Rainier", performs a single mathematical operation, discrete optimization. Rainier uses quantum annealing to solve optimization problems. The D-Wave One is claimed to be the world's first commercially available quantum computer system.[37] The price is approximately US$10,000,000.[3]
A research team led by Matthias Troyer and Daniel Lidar found that, while there is evidence of quantum annealing in D-Wave One, they saw no speed increase compared to classical computers. They implemented an optimized classical algorithm to solve the same particular problem as the D-Wave One.


D-Wave Two[edit]
Main article: D-Wave Two
In early 2012, D-Wave Systems revealed a 512-qubit quantum computer, code-named Vesuvius,[43] which was launched as a production processor in 2013.[44]
In May 2013, Catherine McGeoch, a consultant for D-Wave, published the first comparison of the technology against regular top-end desktop computers running an optimization algorithm. Using a configuration with 439 qubits, the system performed 3,600 times as fast as CPLEX, the best algorithm on the conventional machine, solving problems with 100 or more variables in half a second compared with half an hour. The results are presented at the Computing Frontiers 2013 conference.[45]
In March 2013 several groups of researchers at the Adiabatic Quantum Computing workshop at the Institute of Physics in London produced evidence, though only indirect, of quantum entanglement in the D-Wave chips.[46]
In May 2013 it was announced that a collaboration between NASA, Google and the USRA launched a Quantum Artificial Intelligence Lab at the NASA Advanced Supercomputing Division at Ames Research Center in California, using a 512-qubit D-Wave Two that would be used for research into machine learning, among other fields of study


D-Wave 2X and D-Wave 2000Q[edit]
On August 20, 2015, D-Wave released general availability of their D-Wave 2X computer, with 1,000 qubits in a Chimera graph architecture (although, due to magnetic offsets and manufacturing variability inherent in the superconductor circuit fabrication fewer than 1,152 qubits are functional and available for use. The exact number of qubits yielded will vary with each specific processor manufactured.) This was accompanied by a report comparing speeds with high-end single threaded CPUs.[48] Unlike previous reports, this one explicitly stated that question of quantum speedup was not something they were trying to address, and focused on constant-factor performance gains over classical hardware. For general-purpose problems, a speedup of 15x was reported, but it is worth noting that these classical algorithms benefit efficiently from parallelization—so that the computer would be performing on par with, perhaps, 30 high-end single-threaded cores.
The D-Wave 2X processor is based on a 2,048-qubit chip with half of the qubits disabled; these were activated in the D-Wave 2000Q

Comparison of D-Wave systems[edit]

D-Wave One
D-Wave Two
D-Wave 2X
D-Wave 2000Q[51][52]
Release date
May 2011
May 2013
August 2015
January 2017
Code-name
Rainier
Vesuvius
?
?
Qubits
128
512
1152
2048
Couplers[53]
352
1472
3360
6016
Josephson junctions
24,000
?
128,000
128,000
I/O lines / Control lines
?
192
192
200[54]
Operating temperature (K)
?
0.02
0.015
0.015
Power consumption (kW)
?
15.5
25
25
Buyers
Lockheed Martin
Lockheed Martin
Lockheed Martin
Temporal Defense Systems
Google/NASA/USRA
Google/NASA/USRA
Google/NASA/USRA[55]
Los Alamos National Laboratory


Quantum algorithm
—————————
  - Shor's algorithm, named after mathematician Peter Shor, is a quantum algorithm (an algorithm that runs on a quantum computer) for integer factorization formulated in 1994. Informally, it solves the following problem: given an   integer N, find its prime factors.

   -  Grover's algorithm for searching an unstructured database or an unordered list

   - In 2001, researchers demonstrated Shor's algorithm to factor 15 using a 7-qubit NMR computer.[68]


It has also facilitated research on new cryptosystems that are secure from quantum computers, collectively called post-quantum cryptography


Problems that can be addressed with Grover's algorithm have the following properties:
	1.	There is no searchable structure in the collection of possible answers,
	2.	The number of possible answers to check is the same as the number of inputs to the algorithm, and
	3.	There exists a boolean function which evaluates each input and determines whether it is the correct answer
	4.	


 Google announced in 2017 that it expected to achieve quantum supremacy by the end of the year, and IBM says that the best classical computers will be beaten on some task within about five years


Big Bang Theory
————————
Detailed measurements of the expansion rate of the universe place the Big Bang at around 13.8 billionyears ago, which is thus considered the age of the universe


Dark matter is a hypothetical form of matter that is thought to account for approximately 85% of the matter in the universe, and about a quarter of its total energy density. The majority of dark matter is thought to be non-baryonic in nature, possibly being composed of some as-yet undiscovered subatomic particles.[note 1] Its presence is implied in a variety of astrophysicalobservations, including gravitational effects that cannot be explained unless more matter is present than can be seen. For this reason, most experts think dark matter to be ubiquitous in the universe and to have had a strong influence on its structure and evolution. The name dark matter refers to the fact that it does not appear to interact with observable electromagnetic radiation, such as light, and is thus invisible (or 'dark') to the entire electromagnetic spectrum, making it extremely difficult to detect using usual astronomical equipment


he presented his new idea that the universe is expanding and provided the first observational estimation of what is known as the Hubble constant.

The majority of atoms produced by the Big Bang were hydrogen, along with helium and traces of lithium.

The earliest and most direct observational evidence of the validity of the theory are the expansion of the universe according to Hubble's law (as indicated by the redshifts of galaxies), discovery and measurement of the cosmic microwave background and the relative abundances of light elements produced by Big Bang nucleosynthesis. More recent evidence includes observations of galaxy formation and evolution, and the distribution of large-scale cosmic structures,[74] These are sometimes called the "four pillars" of the Big Bang theory.[75]



Game Theory
————————	

Modern game theory began with the idea regarding the existence of mixed-strategy equilibria in two-person zero-sum games and its proof by John von Neumann. Von Neumann's original proof used the Brouwer fixed-point theorem on continuous mappings into compact convex sets, which became a standard method in game theory and mathematical economics.


His paper was followed by the 1944 book Theory of Games and Economic Behavior, co-written with Oskar Morgenstern, which considered cooperative games of several players

 As of 2014, with the Nobel Memorial Prize in Economic Sciences going to game theorist Jean Tirole, eleven game theorists have won the economics Nobel Prize. John Maynard Smith was awarded the Crafoord Prize for his application of game theory to biology.

In this letter, Waldegrave provides a minimax mixed strategy solution to a two-person version of the card game le Her, and the problem is now known as Waldegrave problem

James Madison made what we now recognize as a game-theoretic analysis of the ways states can be expected to behave under different systems of taxation

In his 1838 Recherches sur les principes mathématiques de la théorie des richesses (Researches into the Mathematical Principles of the Theory of Wealth), Antoine Augustin Cournot considered a duopoly and presents a solution that is a restricted version of the Nash equilibrium.

In game theory, the Nash equilibrium, named after the late American mathematician John Forbes Nash Jr., is a solution conceptof a non-cooperative game involving two or more players in which each player is assumed to know the equilibrium strategies of the other players, and no player has anything to gain by changing only their own strategy.[1] If each player has chosen a strategy and no player can benefit by changing strategies while the other players keep theirs unchanged, then the current set of strategy choices and the corresponding payoffs constitutes a Nash equilibrium. The Nash equilibrium is one of the foundational concepts in game theory. The reality of the Nash equilibrium of a game can be tested using experimental economicsmethods

In game theory, the Nash equilibrium, named after the late American mathematician John Forbes Nash Jr., is a solution conceptof a non-cooperative game involving two or more players in which each player is assumed to know the equilibrium strategies of the other players, and no player has anything to gain by changing only their own strategy.[1] If each player has chosen a strategy and no player can benefit by changing strategies while the other players keep theirs unchanged, then the current set of strategy choices and the corresponding payoffs constitutes a Nash equilibrium. The Nash equilibrium is one of the foundational concepts in game theory. The reality of the Nash equilibrium of a game can be tested using experimental economicsmethods	


Game theory did not really exist as a unique field until John von Neumann published the paper On the Theory of Games of Strategy in 1928.


 Nash proved that every n-player, non-zero-sum (not just 2-player zero-sum) non-cooperative game has what is now known as a Nash equilibrium.

In game theory, a subgame perfect equilibrium (or subgame perfect Nash equilibrium) is a refinement of a Nash equilibriumused in dynamic games

In 1965, Reinhard Selten introduced his solution concept of subgame perfect equilibria, which further refined the Nash equilibrium (later he would introduce trembling hand perfection as well). In 1994 Nash, Selten and Harsanyi became Economics Nobel Laureates for their contributions to economic game theory.

In the 1970s, game theory was extensively applied in biology, largely as a result of the work of John Maynard Smith and his evolutionarily stable strategy. In addition, the concepts of correlated equilibrium, trembling hand perfection, and common knowledge[12] were introduced and analyzed.

In 2005, game theorists Thomas Schelling and Robert Aumann followed Nash, Selten and Harsanyi as Nobel Laureates. Schelling worked on dynamic models, early examples of evolutionary game theory. Aumann contributed more to the equilibrium school, introducing an equilibrium coarsening, correlated equilibrium, and developing an extensive formal analysis of the assumption of common knowledge and of its consequences.

In 2012, Alvin E. Roth and Lloyd S. Shapley were awarded the Nobel Prize in Economics "for the theory of stable allocations and the practice of market design" and, in 2014, the Nobel went to game theorist Jean Tirole

One example would be Peter John Wood's (2013) research when he looked into what nations could do to help reduce climate change. Wood thought this could be accomplished by making treaties with other nations to reduce green house gas emissions. However, he concluded that this idea could not work because it would create a prisoner's dilemma to the nations

According to Maynard Smith, in the preface to Evolution and the Theory of Games, "paradoxically, it has turned out that game theory is more readily applied to biology than to the field of economic behaviour for which it was originally designed". Evolutionary game theory has been used to explain many seemingly incongruous phenomena in nature


Turing machine
——————————

A Turing machine is equivalent to a single-stack pushdown automaton (PDA) that has been made more flexible and concise by relaxing the last-in-first-out requirement of its stack.

A limitation of Turing machines is that they do not model the strengths of a particular arrangement well. For instance, modern stored-program computers are actually instances of a more specific form of abstract machine known as the random-access stored-program machine or RASP machine model


AI/ML
———

Warren McCulloch and Walter Pitts[2] (1943) created a computational model for neural networks based on mathematics and algorithms called threshold logic. This model paved the way for neural network research to split into two approaches. One approach focused on biological processes in the brain while the other focused on the application of neural networks to artificial intelligence. This work led to work on nerve networks and their link to finite automata.[3]


This is a learning method specially designed for cerebellar model articulation controller (CMAC) neural networks.

A convolutional neural network (CNN) is a class of deep, feed-forward networks, composed of one or more convolutional layers with fully connected layers (matching those in typical Artificial neural networks) on top.
Recurrent Neural Network(RNN) has loop in it.	


A recent development has been that of Capsule Neural Network (CapsNet), the idea behind which is to add structures called capsules to a CNN and to reuse output from several of those capsules to form more stable (with respect to various perturbations) representations for higher order capsules.[102]

Long short-term memory (LSTM) networks are RNNs that avoid the vanishing gradient problem.[

Stacks of LSTM RNNs[106] trained by Connectionist Temporal Classification (CTC)[107] can find an RNN weight matrix that maximizes the probability of the label sequences in a training set, given the corresponding input sequences. CTC achieves both alignment and recognition.

Deep Reservoir Computing and Deep Echo State Networks (deepESNs)[123][124] provide a framework for efficiently trained models for hierarchical processing of temporal data, while enabling the investigation of the inherent role of RNN layered composition.

A deep belief network (DBN) is a probabilistic, generative model made up of multiple layers of hidden units. It can be considered a composition of simple learning modules that make up each laye


Large memory storage and retrieval neural networks (LAMSTAR)[127][128] are fast deep learning neural networks of many layers that can use many filters simultaneously. These filters may be nonlinear, stochastic, logic, non-stationary, or even non-analytical. They are biologically motivated and learn continuously	

The auto encoder idea is motivated by the concept of a good representation. For example, for a classifier, a good representation can be defined as one that yields a better-performing classifier.


A deep stacking network (DSN)[147] (deep convex network) is based on a hierarchy of blocks of simplified neural network modules. It was introduced in 2011 by Deng and Dong

A deep predictive coding network (DPCN) is a predictive coding scheme that uses top-down information to empirically adjust the priors needed for a bottom-up inferenceprocedure by means of a deep, locally connected, generative model. 

DPCNs can be extended to form a convolutional network

Integrating external memory with Artificial neural networks dates to early research in distributed representations[174] and Kohonen's self-organizing maps.

Neural Turing machines[182] couple LSTM networks to external memory resources, with which they can interact by attentional processes.

Differentiable neural computers (DNC) are an NTM extension. They out-performed Neural turing machines, long short-term memory systems and memory networks on sequence-processing tasks.[183][184][185][186][187]

Semantic hashing - Approaches that represent previous experiences directly and use a similar experience to form a local model are often called nearest neighbour or k-nearest neighborsmethods

Memory networks - Memory networks[191][192] are another extension to neural networks incorporating long-term memory. The long-term memory can be read and written to, with the goal of using it for prediction. These models have been applied in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base and the output is a textual response.[193] A team of electrical and computer engineers from UCLA Samueli School of Engineering has created a physical artificial neural network. That can analyze large volumes of data and identify objects at the actual speed of light.

Pointer networks[edit]
Deep neural networks can be potentially improved by deepening and parameter reduction, while maintaining trainability. While training extremely deep (e.g., 1 million layers) neural networks might not be practical, CPU-like architectures such as pointer networks[195] and neural random-access machines[196] overcome this limitation by using external random-access memory and other components that typically belong to a computer architecture such as registers, ALU and pointers. Such systems operate on probability distribution vectors stored in memory cells and registers. Thus, the model is fully differentiable and trains end-to-end. The key characteristic of these models is that their depth, the size of their short-term memory, and the number of parameters can be altered independently – unlike models like LSTM, whose number of parameters grows quadratically with memory size.


Encoder–decoder networks[edit]
Encoder–decoder frameworks are based on neural networks that map highly structured input to highly structured output. The approach arose in the context of machine translation,[197][198][199] where the input and output are written sentences in two natural languages. In that work, an LSTM RNN or CNN was used as an encoder to summarize a source sentence, and the summary was decoded using a conditional RNN language model to produce the translation.[200] These systems share building blocks: gated RNNs and CNNs and trained attention mechanisms.


Multilayer kernel machine[edit]
Multilayer kernel machines (MKM) are a way of learning highly nonlinear functions by iterative application of weakly nonlinear kernels. They use the kernel principal component analysis (KPCA),[201] as a method for the unsupervised greedy layer-wise pre-training step of deep learning


Multilayer kernel machines (MKM) are a way of learning highly nonlinear functions by iterative application of weakly nonlinear kernels.

Neural architecture search (NAS) uses machine learning to automate the design of Artificial neural networks.

Recurrent neural networks were developed in the 1980s. Hopfield networks were discovered by John Hopfield in 1982. In 1993, a neural history compressor system solved a "Very Deep Learning" task that required more than 1000 subsequent layers in an RNN unfolded in time.[5]

An Elman network is a three-layer network (arranged horizontally as x, y, and z in the illustration) with the addition of a set of "context units" (u in the illustration)

Elman and Jordan networks are also known as "simple recurrent networks" (SRN).

The Hopfield network is an RNN in which all connections are symmetric

Bidirectional associative memory[edit]
Main article: Bidirectional associative memory
Introduced by Bart Kosko,[21] a bidirectional associative memory (BAM) network is a variant of a Hopfield network that stores associative data as a vector.

Echo state[edit]
Main article: Echo state network
The echo state network (ESN) has a sparsely connected random hidden layer

Independent RNN (IndRNN)[edit]
The Independently recurrent neural network (IndRNN)[26] addresses the gradient vanishing and exploding problems in the traditional fully connected RNN

Recursive[edit]
Main article: Recursive neural network
A recursive neural network[27] is created by applying the same set of weights recursively over a differentiable graph-like structure by traversing the structure in topological order

Neural history compressor[edit]
The neural history compressor is an unsupervised stack of RNNs


Second order RNNs[edit]
Second order RNNs use higher order weights w(i,j,k)  instead of the standard  w(i,j) weights, and states can be a product

Long short-term memory[edit]
Main article: Long short-term memory


Long short-term memory unit
Long short-term memory (LSTM) is a deep learning system that avoids the vanishing gradient problem

Gated recurrent unit[edit]
Main article: Gated recurrent unit


Gated recurrent unit
Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks introduced in 2014

Bi-directional[edit]
Bi-directional RNNs use a finite sequence to predict or label each element of the sequence based on the element's past and future contexts

Continuous-time[edit]
A continuous time recurrent neural network (CTRNN) uses a system of ordinary differential equations to model the effects on a neuron of the incoming spike train.

CTRNNs have been applied to evolutionary robotics where they have been used to address vision,[48] co-operation,[49] and minimal cognitive behaviour

Hierarchical[edit]
Hierarchical RNNs connect their neurons in various ways to decompose hierarchical behavior into useful subprograms


Recurrent multilayer perceptron network[edit]
Generally, a Recurrent Multi-Layer Perceptron (RMLP) network consists of cascaded subnetworks, each of which contains multiple layers of nodes. 


Multiple timescales model[edit]
A multiple timescales recurrent neural network (MTRNN) is a neural-based computational model that can simulate the functional hierarchy of the brain through self-organization that depends on spatial connection between neurons and on distinct types of neuron activities, each with distinct time properties


Neural Turing machines[edit]
Main article: Neural Turing machine
Neural Turing machines (NTMs) are a method of extending recurrent neural networks by coupling them to external memory resources which they can interact with by attentional processes


Differentiable neural computer[edit]
Main article: Differentiable neural computer
Differentiable neural computers (DNCs) are an extension of Neural Turing machines, allowing for usage of fuzzy amounts of each memory address and a record of chronology


Neural network pushdown automata[edit]
Neural network pushdown automata (NNPDA) are similar to NTMs, but tapes are replaced by analogue stacks that are differentiable and that are trained


The most common global optimization method for training RNNs is genetic algorithms, especially in unstructured networks.

RNNs may behave chaotically. In such cases, dynamical systems theory may be used for analysis.

RNNs may behave chaotically. In such cases, dynamical systems theory may be used for analysis.

LeNet-5, a pioneering 7-level convolutional network by LeCun et al. in 1998,[17] that classifies digits, was applied by several banks to recognise hand-written numbers on checks (cheques) digitized in 32x32 pixel images

The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object classification and detection, with millions of images and hundreds of object classes. In the ILSVRC 2014,[54] a large-scale visual recognition challenge, almost every highly ranked team used CNN as their basic framework. The winner GoogLeNet[55] (the foundation of DeepDream) increased the mean average precision of object detection to 0.439329, and reduced classification error to 0.06656, the best result to date. Its network applied more than 30 layers

In 2015, Atomwise introduced AtomNet, the first deep learning neural network for structure-based rational drug design.

Deep Q-networks[edit]
A deep Q-network (DQN) is a type of deep learning model that combines a deep CNN with Q-learning, a form of reinforcement learning.


Deep belief networks[edit]
Main article: Deep belief network
Convolutional deep belief networks (CDBN) have structure very similar to convolutional neural networks and are trained similarly to deep belief networks
	
Time delay neural networks[edit]
A time delay neural network allows speech signals to be processed time-invariantly, analogous to the translation invariance offered by CNNs

Sigmoid function - activation function

Logistic regression was developed by statistician David Cox in 1958.

Likelihood ratio test[edit]
The likelihood-ratio test discussed above to assess model fit is also the recommended procedure to assess the contribution of individual "predictors" to a given mode


Wald statistic[edit]
Alternatively, when assessing the contribution of individual predictors in a given model, one may examine the significance of the Wald statistic.

Case-control sampling[edit]
Suppose cases are rare. Then we might wish to sample them more frequently than their prevalence in the population.

The original SVM algorithm was invented by Vladimir N. Vapnik and Alexey Ya. Chervonenkis in 1963


The original maximum-margin hyperplane algorithm proposed by Vapnik in 1963 constructed a linear classifier. However, in 1992, Bernhard E. Boser, Isabelle M. Guyon and Vladimir N. Vapnik suggested a way to create nonlinear classifiers by applying the kernel trick (originally proposed by Aizerman et al.[14]) to maximum-margin hyperplanes

Support vector clustering (SVC)[edit]
SVC is a similar method that also builds on kernel functions but is appropriate for unsupervised learning. It is considered a fundamental method in data science


Multiclass SVM[edit]
Multiclass SVM aims to assign labels to instances by using support vector machines, where the labels are drawn from a finite set of several elements.

Random forest terms - Tree bagging

A relationship between random forests and the k-nearest neighbor algorithm (k-NN) was pointed out by Lin and Jeon in 2002

Reinforcement learning terms - Value function, Monte Carlo methods, policy, Temporal difference methods, Direct policy search


Algorithm
Description
Model
Policy
Action Space
State Space
Operator
Monte Carlo
Every visit to Monte Carlo
Model-Free
Off-policy
Discrete
Discrete
Sample-means
Q-learning
State–action–reward–state
Model-Free
Off-policy
Discrete
Discrete
Q-value
SARSA
State–action–reward–state–action
Model-Free
On-policy
Discrete
Discrete
Q-value
Q-learning - Lambda
State–action–reward–state with eligibility traces
Model-Free
Off-policy
Discrete
Discrete
Q-value
SARSA - Lambda
State–action–reward–state–action with eligibility traces
Model-Free
On-policy
Discrete
Discrete
Q-value
DQN
Deep Q Network
Model-Free
Off-policy
Discrete
Continuous
Q-value
DDPG
Deep Deterministic Policy Gradient
Model-Free
Off-policy
Continuous
Continuous
Q-value
A3C
Asynchronous Actor-Critic Algorithm
Model-Free
Off-policy
Continuous
Continuous
Q-value
NAF
Q-Learning with Normalized Advantage Functions
Model-Free
Off-policy
Continuous
Continuous
Advantage
TRPO
Trust Region Policy Optimization
Model-Free
Off-policy
Continuous
Continuous
Advantage
PPO
Proximal Policy Optimization
Model-Free
Off-policy
Continuous
Continuous
Advantage


Automata theory
———————————
The idea originated in the work of Konrad Zuse, and was popularized in America by Edward Fredkin. Automata also appear in the theory of finite fields: the set of irreducible polynomials which can be written as composition of degree two polynomials is in fact a regular language.

One can define several distinct categories of automata[4] following the automata classification into different types described in the previous section. The mathematical category of deterministic automata, sequential machines or sequential automata, and Turing machines with automata homomorphisms defining the arrows between automata is a Cartesian closed category,[


Fibonacci numbers
————————————

Fibonacci numbers appear to have first arisen in perhaps 200 BC in work by Pingala on enumerating possible patterns of poetry formed from syllables of two lengths. The Fibonacci sequence is named after Italian mathematician Leonardo of Pisa, known as Fibonacci.


Cassini's identity and Catalan's identity are mathematical identities for the Fibonacci numbers. The former is a special case of the latter, and states that for the nth Fibonacci number,

Every third number of the sequence is even and more generally, every kth number of the sequence is a multiple of Fk. Thus the Fibonacci sequence is an example of a divisibility sequence

The Fibonacci numbers are important in the computational run-time analysis of Euclid's algorithm to determine the greatest common divisor of two integers: the worst case input for this algorithm is a pair of consecutive Fibonacci numbers

Moreover, every positive integer can be written in a unique way as the sum of one or more distinct Fibonacci numbers in such a way that the sum does not include any two consecutive Fibonacci numbers. This is known as Zeckendorf's theorem, and a sum of Fibonacci numbers that satisfies these conditions is called a Zeckendorf representation

They are also used in planning poker, which is a step in estimating in software development projects that use the Scrum (software development) methodology.



CRYPTO CURRENCY
——————————
The decentralized control of each cryptocurrency works through distributed ledger technology, typically a blockchain, that serves as a public financial transaction database


Bitcoin, first released as open-source software in 2009, is generally considered the first decentralized cryptocurrency.[8] Since the release of Bitcoin, over 4,000 altcoins(alternative variants of Bitcoin, or other cryptocurrencies) have been created.

In 1983 the American cryptographer David Chaum conceived an anonymous cryptographic electronic money called ecash.[9][10] Later, in 1995, he implemented it through Digicash

In 1996 the NSA (National Security Agency) published a paper entitled How to Make a Mint: the Cryptography of Anonymous Electronic Cash, describing a Cryptocurrency system first publishing it in a MIT mailing list[12] and later in 1997, in The American Law Review (Vol. 46, Issue 4)

In 1998, Wei Dai published a description of "b-money", an anonymous, distributed electronic cash system.[14] Shortly thereafter, Nick Szabo created "bit gold".[15] Like bitcoin and other cryptocurrencies that would follow it, bit gold (not to be confused with the later gold-based exchange, BitGold) was an electronic currency system which required users to complete a proof of work function with solutions being cryptographically put together and published. A currency system based on a reusable proof of work was later created by Hal Finney who followed the work of Dai and Szabo

The first decentralized cryptocurrency, bitcoin, was created in 2009 by pseudonymous developer Satoshi Nakamoto. It used SHA-256, a cryptographic hash function, as its proof-of-work scheme.[16][17] In April 2011, Namecoin was created as an attempt at forming a decentralized DNS, which would make internet censorship very difficult. Soon after, in October 2011, Litecoin was released. It was the first successful cryptocurrency to use scrypt as its hash function instead of SHA-256. Another notable cryptocurrency, Peercoin was the first to use a proof-of-work/proof-of-stake hybrid.[18] IOTA was the first cryptocurrency not based on a blockchain, and instead uses the Tangle.[19][20] Many other cryptocurrencies have been created though few have been successful, as they have brought little in the way of technical innovation.[21] On 6 August 2014, the UK announced its Treasury had been commissioned to do a study of cryptocurrencies, and what role, if any, they can play in the UK economy. The study was also to report on whether regulation should be considered.[22]

In March 2018, the word cryptocurrency was added to the Merriam-Webster Dictionary.

Altcoin
The term altcoin has various similar definitions. Stephanie Yang of The Wall Street Journal defined altcoins as "alternative digital currencies,"[25] while Paul Vigna, also of The Wall Street Journal, described altcoins as alternative versions of bitcoin.[26] Aaron Hankins of the MarketWatch refers to any cryptocurrencies other than bitcoin as altcoins

Crypto token
A blockchain account can provide functions other than making payments, for example in decentralized applications or smart contracts. In this case, the units or coins are sometimes referred to as crypto tokens (or cryptotokens).


ordan Kelley, founder of Robocoin, launched the first bitcoin ATM in the United States on 20 February 2014.

On 21 November 2017, the Tether cryptocurrency announced they were hacked, losing $31 million in USDT from their primary wallet.

Cryptocurrency is also used in controversial settings in the form of online black markets, such as Silk Road.


SPECIAL RELATIVITY
——————————

The principle of relativity, which states that physical laws have the same form in each inertial reference frame, dates back to Galileo, and was incorporated into Newtonian physics. However, in the late 19th century, the existence of electromagnetic waves led physicists to suggest that the universe was filled with a substance that they called "aether", which would act as the medium through which these waves, or vibrations travelled

The results of various experiments, including the Michelson–Morley experiment, led to the theory of special relativity, by showing that there was no aether.[

	•	The Fizeau experiment (1851, repeated by Michelson and Morley in 1886) measured the speed of light in moving media, with results that are consistent with relativistic addition of colinear velocities.

	•	The famous Michelson–Morley experiment (1881, 1887) gave further support to the postulate that detecting an absolute reference velocity was not achievable. It should be stated here that, contrary to many alternative claims, it said little about the invariance of the speed of light with respect to the source and observer's velocity, as both source and observer were travelling together at the same velocity at all times.

	•	The Trouton–Noble experiment (1903) showed that the torque on a capacitor is independent of position and inertial reference frame.

	•	The Experiments of Rayleigh and Brace (1902, 1904) showed that length contraction doesn't lead to birefringence for a co-moving observer, in accordance with the relativity principle.

	•	Tests of relativistic energy and momentum – testing the limiting speed of particles
	•	Ives–Stilwell experiment – testing relativistic Doppler effect and time dilation
	•	Experimental testing of time dilation – relativistic effects on a fast-moving particle's half-life
	•	Kennedy–Thorndike experiment – time dilation in accordance with Lorentz transformations
	•	Hughes–Drever experiment – testing isotropy of space and mass
	•	Modern searches for Lorentz violation – various modern tests
	•	Experiments to test emission theory demonstrated that the speed of light is independent of the speed of the emitter.
	•	Experiments to test the aether drag hypothesis – no "aether flow obstruction".

Special relativity can be combined with quantum mechanics to form relativistic quantum mechanics and quantum electrodynamics. It is an unsolved problem in physics how general relativity and quantum mechanics can be unified; quantum gravity and a "theory of everything", which require a unification including general relativity too, are active and ongoing areas in theoretical research.

STRING THEROY
————————
Since string theory incorporates all of the fundamental interactions, including gravity, many physicists hope that it fully describes our universe, making it a theory of everything. 

In quantum field theory, one typically computes the probabilities of various physical events using the techniques of perturbation theory.

Developed by Richard Feynman and others in the first half of the twentieth century, perturbative quantum field theory uses special diagrams called Feynman diagrams to organize computations.

The original version of string theory was bosonic string theory, but this version described only bosons, a class of particles which transmit forces between the matter particles, or fermions

ISSAC ASIMOV
————————


	
NEWS
————

TURING MACHINE
—————————

November 8, 1991
The first Turing Test (a.k.a Loebner Prize Competition) is held at the Boston Computer Museum. Joseph Weintraub, president of Thinking Software, Inc., wins the competition by fooling 5 of the 10 judges into thinking his software, “programmed to make whimsical conversations,” was human

In 2016, Nuance Communications has sponsored the first round of the Winograd Schema Challenge, an alternative to the Turing Test. The results: Machines were 58.33% correct in their pronoun resolution compared to humans at 90.9% accuracy. Still, Google’s “artificial intelligence machine” makes headlines when it becomes “exasperated” and ends a conversation “by lashing out at its human inquisitor.”

November 11, 1997
IBM announces the first high-capacity desktop PC disk drive with a new breakthrough technology called Giant Magnetoresistive (GMR) heads, which enabled the further miniaturization of disk drives. The 2007 Nobel Prize in Physics was awarded to Albert Fert and Peter Grünberg for the discovery of the GMR effect in 1988.

November 12, 1936
Alan Turing delivers to the London Mathematical Society his paper “On Computable Numbers, with an Application to the Entscheidungsproblem.” In the paper, Turing described the Universal Machine, later to be known as the Turing Machine, an idealized computing device that is capable of performing any mathematical computation that can be represented as an algorithm.

Researchers at MIT and University of Pennsylvania have given a legendary test of artificial intelligence a tweak, creating a new ‘Minimal Turing test’.It takes a leaf from what's arguably the benchmark to qualify artificial intelligence, the Turing test, to distinguish an AI from a human.
The new Minimal Turing test simplifies things considerably, only asking: If you could pick just one word to convince someone human that you weren't a robot, which word would you choose?

A Turing Test, named for mathematician Alan Turing, centres on the idea that if a computer was able to trick a human into believing that it was human, then the machine was deemed to be “intelligent” or indistinguishable from people. But passing a ‘Visual’ Turing Test involves computers being able to recognise objects in photographs and other images and “understanding” the relationships or implied activities between them. Researchers have recently devised a method of evaluating how well computers perform at that task. Researchers from Brown and Johns Hopkins Universities, U.S. have come up with a new way to evaluate how well computers can divine information from images.


In 2015, the Allen Institute for Artificial Intelligence (AI2) ran its first Allen AI Science Challenge, a competition to test machines on an ostensibly difficult task—answering eighth-grade science questions. 

''Eugene Goostman'', a computer programme  has duped humans into thinking it is a 13-year-old boy to become the first machine to pass the ''iconic'' Turing Test, experts have said.

A Georgia Tech professor recently offered an alternative to the celebrated "Turing Test" to determine whether a machine or computer program exhibits human-level intelligence
For the test, the artificial agent passes if it develops a creative artifact from a subset of artistic genres deemed to require human-level intelligence and the artifact meets certain creative constraints given by a human evaluator

A serious problem in the Turing test for computer intelligence is exposed in a study published in the Journal of Experimental and Theoretical Artificial Intelligence.if a machine were to 'take the Fifth Amendment' -- that is, exercise the right to remain silent throughout the test -- it could, potentially, pass the test and thus be regarded as a thinking entity, authors Kevin Warwick and Huma Shah of Coventry University argue. However, if this is the case, any silent entity could pass the test, even if it were clearly incapable of thought.

THEORY OF RELATIVITY
——————————————

A team of scientists working with the European Southern Observatory (ESO), in Chile, recently concluded a 26 year study to determine what happens when a star passes by a black hole. The results of the campaign appear to confirm Albert Einstein’s famed theory of general relativity. It turns out, E really does equal MC squared.

Einstein Was Right! Scientists Confirm General Relativity Works With Distant Galaxy
A new study validates Einstein's theory of general relativity in a distant galaxy for the first time.

In the latest test, an international team of astronomers used a giant telescope in Chile to observe a star as it moved through the intense gravitational field of a black hole — and found that light from the star was stretched by the gravity just as Einstein's iconic theory said it would be.


GAME THEORY
————————

University of Warwick
Summary:
Many of the world's major problems are spawned not from monolithic blocks of self-interest, but from a vast array of single entities making highly individual choices: from lone wolf terrorists to corrupt officials, tax evaders, isolated hackers or even armies of botnets and packages of malware. Game theory needs to catch up, and new research by mathematicians has just found the way to do that by giving game theory calculations an enormous army of 'agents'.

Constantinos Daskalakis proved that computing nash equilibrium for a three-game person is computationally intractable.


PRIME NUMBERS
————————
Princeton University researchers have uncovered patterns in prime numbers that are similar to those found in the positions of atoms inside certain crystal-like materials

Read more at: https://phys.org/news/2018-09-hidden-prime-crystal-like-materials.html#jCp

Great Internet Mersenne Prime Search (GIMPS)

The Great Internet Mersenne Prime Search (GIMPS) has discovered the largest known prime number, 277,232,917-1, having 23,249,425 digits. A computer volunteered by Jonathan Pace made the find on December 26, 2017. Jonathan is one of thousands of volunteers using free GIMPS software.
Jon Pace is the discoverer of the largest known prime number, 23.2 million digits long


FIBONACCI NUMBERS
—————————————

Biologists have long suspected that the branching of trees and other occurrences of the Fibonacci sequence in nature is simply a reaction to minimize stress, but no concrete proof of it has every been found

Friedrich-Schiller-Universitaet Jena
Summary:
Bioinformatics scientists have discovered that the number of theoretically possible fatty acids with the same chain length but different structures can be determined with the aid of the famous Fibonacci sequence. The number of possible fatty acids with increasing chain length rises at each step by a factor of approximately 1.618, and therefore agrees with what is called the 'Golden Mean'.
Plasmonic nanowires arranged in Fibonacci number chain: Excitation angle-dependent optical properties

QUANTUM MECHANICS
————————————
Scientists at Radboud University discovered a new mechanism for magnetic storage of information in the smallest unit of matter: a single atom.

Read more at: https://phys.org/news/2018-09-scientists-mechanism-storage-atom.html#jCp

Yale University researchers have demonstrated one of the key steps in building the architecture for modular quantum computers: the “teleportation” of a quantum gate between two qubits, on demand. The findings appear online September 5 in the journal Nature.

Physicists have created one precondition for using quantum cryptography to communicate over large distances as well without any risk of interception.

A team of researchers from MIT, Google, the University of Sydney, and Cornell University present a new quantum error correcting code that requires measurements of only a few quantum bits at a time to ensure consistency between one stage of a computation and the next.

Researchers give theoretical proof that quantum mechanics may lead to an ultra-secure internet.
Einstein’s skepticism about quantum mechanics may lead to an ultra-secure internet suggests a new paper by researchers from Swinburne University of Technology and Peking University.
Researchers Perform Logic Operation and Error Correction in a Quantum Register

NASA researchers plan to create the coldest spot in the known Universe–inside the International Space Station. The device, known as the Cold Atom Lab, could discover new forms of matter and novel quantum phenomena.

In a new study, an international team of researchers mathematically describe how an expanding universe can emerge when time and space are heated, without requiring anything like a “Big Bang”.

When soup is heated, it starts to boil. When time and space are heated, an expanding universe can emerge, without requiring anything like a “Big Bang”. This phase transition between a boring empty space and an expanding universe containing mass has now been mathematically described by a research team at the Vienna University of Technology, together with colleagues from Harvard, the MITand Edinburgh. The idea behind this result is a remarkable connection between quantum field theory and Einstein’s theory of relativity.

A newly published study shows that it is possible to clone quantum information from the past, allowing for a particle, or a time traveler, to make multiple loops back in time

In a newly published study, an MIT physicist details how the creation of two entangled quarks simultaneously gives rise to a wormhole connecting the pair. The theoretical results support the idea that the laws of gravity holding together the universe may not be fundamental, but arise from quantum entanglement.

A breakthrough in quantum cryptography demonstrates that information can be encrypted and then decrypted with complete security using the combined power of quantum theory and relativity – allowing the sender to dictate the unveiling of coded information without any possibility of intrusion or manipulation.


NASA funding will allow quantum mechanics investigators to test Herbert Bernsten’s SuperDense quantum teleportation theory from the International Space Station.

In a newly published study, researchers present a method for wireless data transmission at a world-record rate of 100 gigabits per second.

By binding photons together to form molecules, scientists from Harvard and MIT have created a never-before-seen form of matter.s

This fundamental equation, which describes the expanding universe, was derived by the Russian mathematician Alexander Friedman in the 1920s on the basis of the General Theory of Relativity.

Physicists from the Max Planck Institute and the Perimeter Institute in Canada have developed a new approach to the unification of the general theory of relativity and quantum theory.

Recent analyses from a team of physicists at University of Warsaw suggest that not all elementary particles are subject to the same space-time.

In a newly published study, researchers from the Niels Bohr Institute demonstrate the deterministic continuous-variable quantum teleportation of information between two clouds of gas atoms.

In a newly published study, physicists take singularity out of black holes, suggesting that black holes could serve as portals to other universe.

In a newly published study, researchers from the University of Vienna detail a new model of a quantum computer, the boson sampling computer.

The latest data presented by scientists on Higgs boson shows that separate measurements of its properties are showing two slightly different masses.
Experiment Using Photons Could Detect Quantum-Scale Black Holes
A newly published study describes the experiment scientists from the Niels Bohr Institute used to demonstrate that light can have both an electric and a magnetic field, but not at the same time.


CRYPTO  CURRENCY
————————————

	•	Bitcoin: Bitcoin was the first and is the most commonly traded cryptocurrency to date.  The currency was developed by Satoshi Nakamoto in 2009, a mysterious figure who developed its blockchain. It has a market capitalisation of around $128 billion billion as of May 2018. 
	•	Ethereum: Developed in 2015, ether is the currency token used in the ethereum blockchain, the second most popular and valuable cryptocurrency. Ether has a market capitalisation of around $56 billion as of May 2018. However, ether has had a turbulent journey. After a major hack in 2016 it split into two currencies, while its value at one stage it reached as high as $1,300 but it has previously crashed briefly to as low as 10 cents. It has proved hugely popular as a launch pad for other cryptocurrencies in 2017, which use the ethereum blockchain's code.
	•	Ripple: Ripple is another distributed ledger system that was founded in 2012. Ripple can be used to track more kinds of transactions, not just of the cryptocurrency. The company behind it has worked with banks and financial institutions, including Santander. It has a market capitalisation of around $24 billion.
	•	Litecoin: This currency is most similar in form to bitcoin, but has moved more quickly to develop new innovations, including faster payments and processes to allow many more transactions. The total value of all Litecoin is around $6 billion.


AI and ROBOTICS
—————————

Researchers have demonstrated holonomic quantum gates under zero-magnetic field at room temperature, which will enable the realization of fast and fault-tolerant universal quantum computers.

Researchers at Caltech have developed an artificial neural network made out of DNA that can solve a classic machine learning problem: correctly identifying handwritten numbers. The work is a significant step in demonstrating the capacity to program artificial intelligence into synthetic biomolecular circuits.


Extracting information quickly from quantum states is necessary for future quantum processors and super-sensitive detectors in existing technologies. Researchers demonstrate a new method that combines quantum phenomena and machine learning to realise a magnetometer with precision beyond the standard quantum limit.

Computer scientists from the University of Bonn have developed software that can look a few minutes into the future: The program first learns the typical sequence of actions, such as cooking, from video sequences. Based on this knowledge, it can then accurately predict in new situations what the chef will do at which point in time. Researchers will present their findings at the world's largest Conference on Computer Vision and Pattern Recognition, which will be held June 19-21 in Salt Lake City, USA.

Engineers have created a novel actuating material that can be powered by visible light, electricity, and other stimuli, and which may replace traditional bulky motors and pneumatic actuators with ones similar to mammalian skeletal muscles in the future.

Developments in artificial intelligence may help us to predict the probability of life on other planets, according to new work by a team based at Plymouth University. The study uses artificial neural networks (ANNs) to classify planets into five types, estimating a probability of life in each case, which could be used in future interstellar exploration missions. The work is presented at the European Week of Astronomy and Space Science (EWASS) in Liverpool on 4 April by Mr Christopher Bishop.

Simons Foun

Summary:
The same techniques used to train self-driving cars and chess-playing computers are now helping physicists explore the complexities of the quantum world. For the first time, physicists have demonstrated that machine learning can reconstruct a quantum system based on relatively few experimental measurements. This method will allow scientists to thoroughly probe systems of particles exponentially faster than conventional, brute-force techniques.












